package llm

import (
	"context"
	"fmt"
	"time"

	"github.com/charmbracelet/log"
	"cookrag-go/internal/models"
	"cookrag-go/internal/observability"
)

// Provider LLMæä¾›è€…æ¥å£
type Provider interface {
	Generate(ctx context.Context, prompt string) (string, error)
	GenerateWithStream(ctx context.Context, prompt string) (<-chan string, error)
}

// Generator LLMç”Ÿæˆå™¨
type Generator struct {
	provider Provider
}

// NewGenerator åˆ›å»ºç”Ÿæˆå™¨
func NewGenerator(provider Provider) *Generator {
	return &Generator{
		provider: provider,
	}
}

// GenerateAnswer ç”Ÿæˆç­”æ¡ˆ
func (g *Generator) GenerateAnswer(ctx context.Context, query string, documents []models.Document) (string, error) {
	// åˆ›å»ºé“¾è·¯è¿½è¸ª span
	span := observability.GlobalTracer.StartSpan(ctx, "llm_generate_answer", map[string]interface{}{
		"query":          query,
		"doc_count":      len(documents),
		"provider":       "llm",
	})
	defer span.End()

	startTime := time.Now()

	log.Infof("ğŸ¤– Generating answer for query: %s", query)
	log.Infof("ğŸ“š Using %d context documents", len(documents))

	// æ„å»ºä¸Šä¸‹æ–‡
	context := g.buildContext(documents)

	// æ„å»ºæç¤ºè¯
	prompt := g.buildPrompt(query, context)

	// è°ƒç”¨LLMç”Ÿæˆ
	answer, err := g.provider.Generate(ctx, prompt)
	if err != nil {
		span.SetError(err)
		return "", fmt.Errorf("LLM generation failed: %w", err)
	}

	latency := float64(time.Since(startTime).Milliseconds())
	span.AddMetadata("latency_ms", latency)
	span.AddMetadata("answer_length", len(answer))
	span.AddMetadata("prompt_length", len(prompt))

	log.Infof("âœ… Answer generated successfully")
	return answer, nil
}

// GenerateAnswerWithStream æµå¼ç”Ÿæˆç­”æ¡ˆ
func (g *Generator) GenerateAnswerWithStream(ctx context.Context, query string, documents []models.Document) (<-chan string, error) {
	log.Infof("ğŸ¤– Generating streaming answer for query: %s", query)

	// æ„å»ºä¸Šä¸‹æ–‡
	context := g.buildContext(documents)

	// æ„å»ºæç¤ºè¯
	prompt := g.buildPrompt(query, context)

	// è°ƒç”¨LLMæµå¼ç”Ÿæˆ
	stream, err := g.provider.GenerateWithStream(ctx, prompt)
	if err != nil {
		return nil, fmt.Errorf("LLM stream generation failed: %w", err)
	}

	return stream, nil
}

// buildContext æ„å»ºä¸Šä¸‹æ–‡
func (g *Generator) buildContext(documents []models.Document) string {
	if len(documents) == 0 {
		return "æ²¡æœ‰æ‰¾åˆ°ç›¸å…³æ–‡æ¡£ã€‚"
	}

	context := "å‚è€ƒæ–‡æ¡£ï¼š\n\n"
	for i, doc := range documents {
		context += fmt.Sprintf("[æ–‡æ¡£%d] %s\n", i+1, doc.Content)
		if doc.Metadata != nil {
			if source, ok := doc.Metadata["source"].(string); ok {
				context += fmt.Sprintf("æ¥æº: %s\n", source)
			}
		}
		context += "\n"
	}

	return context
}

// buildPrompt æ„å»ºæç¤ºè¯
func (g *Generator) buildPrompt(query string, context string) string {
	prompt := fmt.Sprintf(`ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„é—®ç­”åŠ©æ‰‹ã€‚è¯·æ ¹æ®ä»¥ä¸‹å‚è€ƒæ–‡æ¡£å›ç­”ç”¨æˆ·çš„é—®é¢˜ã€‚

å‚è€ƒæ–‡æ¡£ï¼š
%s

é—®é¢˜ï¼š%s

è¦æ±‚ï¼š
1. åŸºäºå‚è€ƒæ–‡æ¡£å›ç­”é—®é¢˜
2. å¦‚æœå‚è€ƒæ–‡æ¡£ä¸­æ²¡æœ‰ç›¸å…³ä¿¡æ¯ï¼Œè¯·æ˜ç¡®è¯´æ˜
3. å›ç­”è¦å‡†ç¡®ã€ç®€æ´ã€æ˜“æ‡‚
4. å¿…è¦æ—¶å¯ä»¥å¼•ç”¨å‚è€ƒæ–‡æ¡£ä¸­çš„å…·ä½“å†…å®¹

å›ç­”ï¼š`, context, query)

	return prompt
}
