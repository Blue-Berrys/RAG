package llm

import (
	"context"
	"fmt"
	"io"
	"net/http"
	"os"
	"time"

	"github.com/charmbracelet/log"
	"github.com/cloudwego/eino-ext/components/model/openai"
	"github.com/cloudwego/eino/components/model"
	"github.com/cloudwego/eino/schema"
	"cookrag-go/internal/observability"
)

// ZhipuLLM æ™ºè°±AI LLMå®ç°ï¼ˆä½¿ç”¨ eino æ¡†æ¶ï¼‰
type ZhipuLLM struct {
	chatModel model.ChatModel
	model     string
}

// NewZhipuLLM åˆ›å»ºæ™ºè°±AI LLMï¼ˆä½¿ç”¨ eino æ¡†æ¶ï¼‰
func NewZhipuLLM(model string) (*ZhipuLLM, error) {
	if model == "" {
		model = "glm-4-flash"
	}

	// ä»ç¯å¢ƒå˜é‡è·å– API Key
	apiKey := os.Getenv("ZHIPU_API_KEY")
	if apiKey == "" {
		return nil, fmt.Errorf("ZHIPU_API_KEY environment variable not set")
	}

	// ä½¿ç”¨ eino-ext çš„ OpenAI ChatModel ç»„ä»¶
	// æ™ºè°±AI æä¾› OpenAI å…¼å®¹æ¥å£
	chatModel, err := openai.NewChatModel(context.Background(), &openai.ChatModelConfig{
		APIKey:     apiKey,
		BaseURL:    "https://open.bigmodel.cn/api/paas/v4",
		Model:      model,
		Timeout:    60 * time.Second,
		HTTPClient: &http.Client{Timeout: 60 * time.Second},
		ByAzure:    false,
	})
	if err != nil {
		return nil, fmt.Errorf("create chat model failed: %w", err)
	}

	return &ZhipuLLM{
		chatModel: chatModel,
		model:     model,
	}, nil
}

// Generate ç”Ÿæˆæ–‡æœ¬
func (z *ZhipuLLM) Generate(ctx context.Context, prompt string) (string, error) {
	// åˆ›å»ºé“¾è·¯è¿½è¸ª span
	span := observability.GlobalTracer.StartSpan(ctx, "zhipu_llm_generate", map[string]interface{}{
		"model":         z.model,
		"prompt_length": len(prompt),
	})
	defer span.End()

	startTime := time.Now()

	log.Infof("ğŸ¤– Zhipu LLM generation: model=%s", z.model)

	// å°† prompt è½¬æ¢ä¸º eino çš„ Message æ ¼å¼
	messages := []*schema.Message{
		schema.UserMessage(prompt),
	}

	// è°ƒç”¨ eino ç”Ÿæˆ
	response, err := z.chatModel.Generate(ctx, messages)
	if err != nil {
		span.SetError(err)
		return "", fmt.Errorf("generate failed: %w", err)
	}

	if response == nil {
		err := fmt.Errorf("no response returned")
		span.SetError(err)
		return "", err
	}

	latency := float64(time.Since(startTime).Milliseconds())
	span.AddMetadata("latency_ms", latency)
	if response != nil && response.Content != "" {
		span.AddMetadata("response_length", len(response.Content))
	}

	log.Infof("âœ… Zhipu LLM generation completed")
	return response.Content, nil
}

// GenerateWithStream æµå¼ç”Ÿæˆ
func (z *ZhipuLLM) GenerateWithStream(ctx context.Context, prompt string) (<-chan string, error) {
	// åˆ›å»ºé“¾è·¯è¿½è¸ª span
	span := observability.GlobalTracer.StartSpan(ctx, "zhipu_llm_stream", map[string]interface{}{
		"model":         z.model,
		"prompt_length": len(prompt),
	})
	defer span.End()

	stream := make(chan string, 10)

	// å°† prompt è½¬æ¢ä¸º eino çš„ Message æ ¼å¼
	messages := []*schema.Message{
		schema.UserMessage(prompt),
	}

	// è°ƒç”¨ eino æµå¼ç”Ÿæˆ
	streamReader, err := z.chatModel.Stream(ctx, messages)
	if err != nil {
		span.SetError(err)
		close(stream)
		return stream, fmt.Errorf("stream generation failed: %w", err)
	}

	// å¯åŠ¨ goroutine å¤„ç†æµå¼å“åº”
	go func() {
		defer close(stream)
		defer streamReader.Close()

		chunkCount := 0
		totalLength := 0

		for {
			chunk, err := streamReader.Recv()
			if err == io.EOF {
				break
			}
			if err != nil {
				log.Warnf("Error reading stream: %v", err)
				break
			}

			if chunk != nil && chunk.Content != "" {
				stream <- chunk.Content
				chunkCount++
				totalLength += len(chunk.Content)
			}
		}

		span.AddMetadata("chunk_count", chunkCount)
		span.AddMetadata("total_length", totalLength)

		log.Infof("âœ… Stream generation completed")
	}()

	return stream, nil
}
